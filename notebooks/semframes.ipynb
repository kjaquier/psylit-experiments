{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Changed cwd: C:\\Users\\kevin\\Documents\\Workspace\\psylit-experiments\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#%% Change working directory from the workspace root to the ipynb file location. Turn this addition off with the DataSciece.changeDirOnImportExport setting\n",
    "import os\n",
    "try:\n",
    "    os.chdir(r'C:\\Users\\kevin\\Documents\\Workspace\\psylit-experiments')\n",
    "    print(\"Changed cwd:\", os.getcwd())\n",
    "except:\n",
    "    print(\"cwd:\", os.getcwd())\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from statistics import mean\n",
    "from os import linesep as EOL\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = 16,10\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tic import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')   # 'Vanilla' spacy model: spacy.load('en_core_web_sm')\n",
    "\n",
    "merge_ents = nlp.create_pipe(\"merge_entities\")\n",
    "\n",
    "nlp.add_pipe(merge_ents, after=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def maybe(f, default=None):\n",
    "    try:\n",
    "        return f()\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "class show_progress:\n",
    "    \n",
    "    def __init__(self, seq, fmt_progress=\"{i} / {n}\", fmt_time=(\" \"*4+\"{t:.5f}s\")):\n",
    "        self.n = maybe(lambda: len(seq))\n",
    "        self.seq = iter(seq)\n",
    "        self.i = 0\n",
    "        self.fmt_progress = fmt_progress\n",
    "        self.fmt_time = fmt_time\n",
    "        self.t = time.perf_counter()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        #print(self.fmt_progress.format(i=self.i, n=self.n))\n",
    "        #self.t = time.perf_counter()\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.t = time.perf_counter() - self.t\n",
    "        if self.i:\n",
    "            print(self.fmt_time.format(t=self.t))\n",
    "        self.i += 1\n",
    "        if self.n and self.i <= self.n:\n",
    "            print(self.fmt_progress.format(i=self.i, n=self.n))\n",
    "        if not self.n:\n",
    "            print(self.i)\n",
    "        self.t = time.perf_counter()\n",
    "        return next(self.seq)\n",
    "        \n",
    "        \n",
    "def human_print_dict(d, tab=' '*2, level=0):\n",
    "    for k, v in d.items():\n",
    "        print(f\"{tab*level}{k}:\", end=' ')\n",
    "        if type(v) == dict:\n",
    "            print(tab*level)\n",
    "            human_print_dict(v, tab, level+1)\n",
    "        else:\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTIONAL_DEPS = ('det','poss','neg','aux','auxpass','ps','mark','ccomp','xcomp','acomp','prt') # TODO spacy equiv of ps\n",
    "\n",
    "def deppaths(doc, skip=(lambda t: t.is_space or t.is_punct), include=(lambda t: not t.is_stop)):\n",
    "\n",
    "    def deppaths_sent(sent):\n",
    "        prev_path_ids = None\n",
    "        for tok in sent:\n",
    "            if tok.n_lefts or tok.n_rights:\n",
    "                continue\n",
    "                \n",
    "            path = [tok] + list(tok.ancestors)\n",
    "            path = list(filter(include, path))\n",
    "            if not path:\n",
    "                continue\n",
    "\n",
    "            yield path\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent = (t for t in sent if not skip(t))\n",
    "        paths = list(deppaths_sent(sent))\n",
    "        \n",
    "        # paths are uniquely identified in the sent by their leaf token\n",
    "        # the last one for a given id is the the complete one\n",
    "        # assumes dicts are ordered (Py 3.6+)\n",
    "        \n",
    "        paths = {p[-1]: p for p in paths}\n",
    "        yield from paths.values()\n",
    "        \n",
    "                \n",
    "def format_sgrams(grams, \n",
    "                  merge_cond=(lambda t: t.dep_ in ('neg','prt')), \n",
    "                  process=(t.lemma_ if t.pos_ in ('VERB','NOUN') else t.text), \n",
    "                  join='-'.join): # merge might include preps\n",
    "    for gram in grams:\n",
    "        n = len(gram)\n",
    "        gram_toks = []\n",
    "        suffixes = []\n",
    "        for t in reversed(gram):\n",
    "            txt = process(t)\n",
    "            if merge_cond(t):\n",
    "                suffixes.insert(0, txt)\n",
    "            else:\n",
    "                tok_txt = join([txt] + suffixes)\n",
    "                gram_toks.append(tok_txt)\n",
    "                suffixes = []\n",
    "                \n",
    "        yield gram_toks\n",
    "        \n",
    "def format_rgrams(grams, process=(t.lemma_ if t.pos_ in ('VERB','NOUN') else t.text)):\n",
    "    # equiv:\n",
    "    # yield from format_sgrams(grams, merge_cond=(lambda t: False))\n",
    "    for gram in grams:\n",
    "        gram_toks = [process(t) for t in gram]\n",
    "        yield gram_toks\n",
    "        \n",
    "\n",
    "class Grams(Sequence):\n",
    "    \n",
    "    def __init__(self, grams):\n",
    "        \"\"\"\n",
    "        grams: Seq[Iterable]\n",
    "        \"\"\"\n",
    "        self.grams = grams\n",
    "    \n",
    "    def map(self, f):\n",
    "        self.grams = map(f, self.grams)\n",
    "    \n",
    "    def merge_from_right(self, match):\n",
    "        def process():\n",
    "            for gram in self.grams:\n",
    "                suffixes = []\n",
    "                for t in reversed(gram):\n",
    "                    if match(t):\n",
    "                        suffixes.insert(0, t)\n",
    "                    else:\n",
    "                        yield [t] + suffixes\n",
    "                        suffixes = []\n",
    "        \n",
    "        return DisjointGrams(process())\n",
    "        \n",
    "        \n",
    "class DisjointGrams(Sequence):\n",
    "    \n",
    "    def __init__(self, grams):\n",
    "        \"\"\"\n",
    "        grams: Seq[Iterable[Iterable]]\n",
    "        \"\"\"\n",
    "        self.grams = grams\n",
    "\n",
    "    def join(self, join):\n",
    "        return Grams(join(g) for g in self.grams)\n",
    "    \n",
    "def ngrams_from_paths(paths, n=2, is_counted=(lambda t: t.dep_ not in FUNCTIONAL_DEPS), step=1):\n",
    "    for p in paths:\n",
    "        yield from ngrams_from_words(p, n, is_counted, step)\n",
    "\n",
    "def ngrams_from_words(toks, n=2, is_counted=(lambda t: t.dep_ not in FUNCTIONAL_DEPS), step=1):\n",
    "    content_idx = [i for i,t in enumerate(toks) if is_counted(t)]\n",
    "    m = len(content_idx)\n",
    "    for j in range(n, m, step):\n",
    "        gram_slice = slice(content_idx[j - n], content_idx[j])\n",
    "        yield toks[gram_slice]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class process_rgrams:\n",
    "    \n",
    "    def __init__(self, doc, nmax=3):\n",
    "        all_words = list(doc)\n",
    "        words = [t for t in all_words if (not t.is_space) and (not t.is_punct) and (not t.is_stop)]\n",
    "        self.reg_grams = [\n",
    "            list(format_rgrams(ngrams_from_words(words, n)))\n",
    "            for n in range(1, nmax+1)\n",
    "        ]\n",
    "        self.nmax = nmax\n",
    "        self.reg_grams_count = [Counter(map(tuple, gs)) for gs in self.reg_grams]\n",
    "\n",
    "        self.stats = {}\n",
    "        \n",
    "    def summary(self):\n",
    "        sm = {}\n",
    "        for n in range(self.nmax):\n",
    "            sm[f'n_reg_{n+1}grams'] = len(self.reg_grams_count[n])\n",
    "        return {**self.stats, **sm}\n",
    "\n",
    "class process_frames:\n",
    "    def __init__(self, doc, nmax=3):\n",
    "        all_words = list(doc)\n",
    "        words = [t for t in all_words if (not t.is_space) and (not t.is_punct) and (not t.is_stop)]\n",
    "        paths = list(deppaths(doc))\n",
    "        self.syn_grams = [\n",
    "            list(format_sgrams(ngrams_from_paths(paths, n)))\n",
    "            for n in range(1, nmax+1)\n",
    "        ]\n",
    "        self.syn_grams_count = [Counter(map(tuple, gs)) for gs in self.syn_grams]\n",
    "        self.nmax = nmax\n",
    "\n",
    "        self.stats = {\n",
    "            'n_dep_paths': len(paths),\n",
    "            'avg_path_length': mean(len(p) for p in paths),\n",
    "        }\n",
    "        \n",
    "    def summary(self):\n",
    "        sm = {}\n",
    "        for n in range(self.nmax):\n",
    "            sm[f'n_syn_{n+1}grams'] = len(self.syn_grams_count[n])\n",
    "        return {**self.stats, **sm}\n",
    "    \n",
    "class process_doc:\n",
    "    \n",
    "    def __init__(self, doc, nmax=3):\n",
    "        all_words = list(doc)\n",
    "        all_sents = list(doc.sents)\n",
    "        ng = process_rgrams(doc, nmax)\n",
    "        sng = process_sngrams(doc, nmax)\n",
    "        self.syn_grams_count = sng.syn_grams_count\n",
    "        self.reg_grams_count = ng.reg_grams_count\n",
    "        \n",
    "        self.stats = {\n",
    "            'n_tokens': len(all_words),\n",
    "            'avg_sentence_length': mean(len(s) for s in all_sents),\n",
    "            'n_sentences': len(all_sents),\n",
    "            **ng.summary(),\n",
    "            **sng.summary(),\n",
    "        }\n",
    "        \n",
    "    def summary(self):\n",
    "        return self.stats\n",
    "    \n",
    "class process_corpus:\n",
    "    \n",
    "    def __init__(self, texts, batch_size=8, disable=None, nmax=3, **kwargs):\n",
    "        disable = disable or []\n",
    "        self.reg_grams_count = [Counter() for _ in range(nmax)]\n",
    "        self.syn_grams_count = [Counter() for _ in range(nmax)]\n",
    "        self.summaries = []\n",
    "        self.suspicious_docs = []\n",
    "        pipeline = nlp.pipe(texts, batch_size=batch_size, disable=disable)\n",
    "        #pipeline = (nlp(txt, disable=disable) for txt in texts)\n",
    "        for doc in show_progress(pipeline):\n",
    "            d = process_doc(doc, **kwargs)\n",
    "            self.summaries.append(d.summary())\n",
    "            \n",
    "            if len(set(t.text for t in d.reg_grams_count[2].keys())) == 1:\n",
    "                self.suspicious_docs.append(doc)\n",
    "            if len(set(t.text for t in d.syn_grams_count[2].keys())) == 1:\n",
    "                self.suspicious_docs.append(doc)\n",
    "            for n in range(nmax):\n",
    "                \n",
    "                self.reg_grams_count[n] += d.reg_grams_count[n]\n",
    "                self.syn_grams_count[n] += d.syn_grams_count[n]\n",
    "        \n",
    "        self.summary = pd.DataFrame(self.summaries)\n",
    "        del self.summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = r'..\\datasets\\2_txtalb_Novel450'\n",
    "files = [os.path.join(data_root, f) for f in os.listdir(data_root) if f.startswith('EN_')][60:70]#110]\n",
    "nfiles = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = process_corpus(preprocess.read_pg(filename)[:nlp.max_length] for filename in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_path_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>n_dep_paths</th>\n",
       "      <th>n_reg_0grams</th>\n",
       "      <th>n_reg_1grams</th>\n",
       "      <th>n_reg_2grams</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_syn_0grams</th>\n",
       "      <th>n_syn_1grams</th>\n",
       "      <th>n_syn_2grams</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.519644</td>\n",
       "      <td>17.066189</td>\n",
       "      <td>9994.140000</td>\n",
       "      <td>11011.620000</td>\n",
       "      <td>40614.580000</td>\n",
       "      <td>44793.900000</td>\n",
       "      <td>9124.120000</td>\n",
       "      <td>4340.660000</td>\n",
       "      <td>6227.300000</td>\n",
       "      <td>3176.520000</td>\n",
       "      <td>154311.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.172444</td>\n",
       "      <td>3.436188</td>\n",
       "      <td>4544.271438</td>\n",
       "      <td>4281.427193</td>\n",
       "      <td>18464.718003</td>\n",
       "      <td>20714.481981</td>\n",
       "      <td>4196.700816</td>\n",
       "      <td>1618.863366</td>\n",
       "      <td>2943.207678</td>\n",
       "      <td>1615.770673</td>\n",
       "      <td>71662.083293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.226048</td>\n",
       "      <td>12.260053</td>\n",
       "      <td>1840.000000</td>\n",
       "      <td>2939.000000</td>\n",
       "      <td>7737.000000</td>\n",
       "      <td>8596.000000</td>\n",
       "      <td>1594.000000</td>\n",
       "      <td>1023.000000</td>\n",
       "      <td>899.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>31418.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.362711</td>\n",
       "      <td>14.378670</td>\n",
       "      <td>5330.750000</td>\n",
       "      <td>6774.250000</td>\n",
       "      <td>22410.000000</td>\n",
       "      <td>24318.500000</td>\n",
       "      <td>5126.250000</td>\n",
       "      <td>2911.250000</td>\n",
       "      <td>3342.500000</td>\n",
       "      <td>1612.750000</td>\n",
       "      <td>79716.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.532044</td>\n",
       "      <td>17.029790</td>\n",
       "      <td>11770.000000</td>\n",
       "      <td>12594.500000</td>\n",
       "      <td>48981.000000</td>\n",
       "      <td>52931.500000</td>\n",
       "      <td>9928.000000</td>\n",
       "      <td>5148.000000</td>\n",
       "      <td>7305.000000</td>\n",
       "      <td>3663.000000</td>\n",
       "      <td>181893.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.618959</td>\n",
       "      <td>19.475784</td>\n",
       "      <td>13507.500000</td>\n",
       "      <td>14615.000000</td>\n",
       "      <td>55597.750000</td>\n",
       "      <td>61393.000000</td>\n",
       "      <td>12349.500000</td>\n",
       "      <td>5609.750000</td>\n",
       "      <td>8568.250000</td>\n",
       "      <td>4357.000000</td>\n",
       "      <td>217958.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.915258</td>\n",
       "      <td>28.908506</td>\n",
       "      <td>17227.000000</td>\n",
       "      <td>16267.000000</td>\n",
       "      <td>63207.000000</td>\n",
       "      <td>69075.000000</td>\n",
       "      <td>17791.000000</td>\n",
       "      <td>6436.000000</td>\n",
       "      <td>10846.000000</td>\n",
       "      <td>6123.000000</td>\n",
       "      <td>231944.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_path_length  avg_sentence_length   n_dep_paths  n_reg_0grams  \\\n",
       "count        50.000000            50.000000     50.000000     50.000000   \n",
       "mean          2.519644            17.066189   9994.140000  11011.620000   \n",
       "std           0.172444             3.436188   4544.271438   4281.427193   \n",
       "min           2.226048            12.260053   1840.000000   2939.000000   \n",
       "25%           2.362711            14.378670   5330.750000   6774.250000   \n",
       "50%           2.532044            17.029790  11770.000000  12594.500000   \n",
       "75%           2.618959            19.475784  13507.500000  14615.000000   \n",
       "max           2.915258            28.908506  17227.000000  16267.000000   \n",
       "\n",
       "       n_reg_1grams  n_reg_2grams   n_sentences  n_syn_0grams  n_syn_1grams  \\\n",
       "count     50.000000     50.000000     50.000000     50.000000     50.000000   \n",
       "mean   40614.580000  44793.900000   9124.120000   4340.660000   6227.300000   \n",
       "std    18464.718003  20714.481981   4196.700816   1618.863366   2943.207678   \n",
       "min     7737.000000   8596.000000   1594.000000   1023.000000    899.000000   \n",
       "25%    22410.000000  24318.500000   5126.250000   2911.250000   3342.500000   \n",
       "50%    48981.000000  52931.500000   9928.000000   5148.000000   7305.000000   \n",
       "75%    55597.750000  61393.000000  12349.500000   5609.750000   8568.250000   \n",
       "max    63207.000000  69075.000000  17791.000000   6436.000000  10846.000000   \n",
       "\n",
       "       n_syn_2grams       n_tokens  \n",
       "count     50.000000      50.000000  \n",
       "mean    3176.520000  154311.800000  \n",
       "std     1615.770673   71662.083293  \n",
       "min      392.000000   31418.000000  \n",
       "25%     1612.750000   79716.750000  \n",
       "50%     3663.000000  181893.500000  \n",
       "75%     4357.000000  217958.000000  \n",
       "max     6123.000000  231944.000000  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Dunbar', 'Dunbar', 'Balderby'), 14),\n",
       " (('office', '-', 'post'), 13),\n",
       " (('bid', 'night', 'good'), 10),\n",
       " (('bid', 'bye', 'good'), 9),\n",
       " (('wish', 'night', 'good'), 8),\n",
       " (('house', 'Dunbar', 'Dunbar'), 8),\n",
       " (('look', 'round', 'room'), 7),\n",
       " (('tête', 'tête', 'à'), 7),\n",
       " (('door', 'room', 'drawing'), 6),\n",
       " (('open', 'door', 'room'), 6),\n",
       " (('till', 'time', 'dinner'), 6),\n",
       " (('window', 'room', 'drawing'), 6),\n",
       " (('look', 'like', 'man'), 6),\n",
       " (('door', 'room', 'dining'), 5),\n",
       " (('finger', 'hand', 'left'), 5),\n",
       " (('door', 'lead', 'room'), 5),\n",
       " (('look', 'shake', 'head'), 5),\n",
       " (('see', 'day', 'better'), 5),\n",
       " (('man', 'woman', 'child'), 5),\n",
       " (('burst', 'flood', 'tear'), 5),\n",
       " (('room', 'floor', 'ground'), 5),\n",
       " (('look', 'look', 'look'), 5),\n",
       " (('look', 'expression', 'face'), 5),\n",
       " (('turn', 'look', 'face'), 4),\n",
       " (('window', 'room', 'dining'), 4),\n",
       " (('like', 'friend', 'old'), 4),\n",
       " (('like', 'beast', 'wild'), 4),\n",
       " (('turn', 'walk', 'away'), 4),\n",
       " (('like', 'child', 'little'), 4),\n",
       " (('wish', 'bye', 'good'), 4),\n",
       " (('enter', 'room', 'drawing'), 4),\n",
       " (('Margaret', 'voice', 'low'), 4),\n",
       " (('horse', '-', 'post'), 4),\n",
       " (('round', 'table', 'breakfast'), 4),\n",
       " (('come', 'room', 'drawing'), 4),\n",
       " (('basket', 'paper', 'waste'), 4),\n",
       " (('woman', 'love', 'dearly'), 4),\n",
       " (('light', 'candle', 'wax'), 4),\n",
       " (('look', 'man', 'young'), 4),\n",
       " (('eye', 'close', 'half'), 4),\n",
       " (('table', 'near', 'fire'), 4),\n",
       " (('speak', 'go', 'away'), 4),\n",
       " (('tone', 'matter', 'fact'), 4),\n",
       " (('draw', 'breath', 'long'), 4),\n",
       " (('find', 'ago', 'long'), 3),\n",
       " (('thing', 'heaven', 'earth'), 3),\n",
       " (('sense', 'feeling', 'right'), 3),\n",
       " (('lay', 'look', 'eye'), 3),\n",
       " (('like', 'flock', 'sheep'), 3),\n",
       " (('like', 'rest', 'world'), 3)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.syn_grams_count[2].most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Mrs.', 'Dormer', 'Smith'), 245),\n",
       " (('said', 'St.', 'Clare'), 148),\n",
       " (('said', 'Miss', 'Ophelia'), 124),\n",
       " (('said', 'Lady', 'Laura'), 119),\n",
       " (('Mrs.', 'Orton', 'Beg'), 116),\n",
       " (('Mrs.', 'Le', 'Marchant'), 97),\n",
       " (('Madame', 'Max', 'Goesler'), 78),\n",
       " (('Mr.', 'Fane', 'Smith'), 78),\n",
       " (('said', 'Mrs.', 'Edmonstone'), 77),\n",
       " (('said', 'Mr.', 'Tulliver'), 75),\n",
       " (('said', 'Mr.', 'Lorry'), 73),\n",
       " (('Mrs.', 'Guthrie', 'Brimston'), 69),\n",
       " (('Sir', 'Percival', 'Glyde'), 67),\n",
       " (('Mr.', 'Dormer', 'Smith'), 66),\n",
       " (('Dr.', 'Van', 'Helsing'), 65),\n",
       " (('said', 'Mrs.', 'Tulliver'), 63),\n",
       " (('said', 'Mr.', 'Jaggers'), 62),\n",
       " (('replied', 'Dr.', 'Leete'), 57),\n",
       " (('said', 'Charles', 'Osmond'), 55),\n",
       " (('said', 'old', 'man'), 54),\n",
       " (('Mrs.', 'Fane', 'Smith'), 54),\n",
       " (('said', 'Mr.', 'Hale'), 52),\n",
       " (('tête', 'à', 'tête'), 52),\n",
       " (('Mr.', 'St.', 'John'), 52),\n",
       " (('said', 'young', 'man'), 49),\n",
       " (('said', 'Lord', 'Henry'), 47),\n",
       " (('said', 'low', 'voice'), 46),\n",
       " (('said', 'Mr', 'Slope'), 45),\n",
       " (('said', 'Mrs.', 'Glegg'), 45),\n",
       " (('said', 'Miss', 'Pross'), 44),\n",
       " (('said', 'Lord', 'Chiltern'), 43),\n",
       " (('Mr.', 'Robert', 'Audley'), 42),\n",
       " (('said', 'Mr.', 'Glegg'), 40),\n",
       " (('St.', 'Gundolph', 'Lane'), 40),\n",
       " (('said', 'Mr.', 'Kennedy'), 40),\n",
       " (('arms', 'round', 'neck'), 38),\n",
       " (('said', 'Aunt', 'Chloe'), 38),\n",
       " (('Mr.', 'Hamilton', 'Wells'), 38),\n",
       " (('Sir', 'Michael', 'Audley'), 37),\n",
       " (('said', 'Madame', 'Merle'), 36),\n",
       " (('DR', 'SEWARD', 'DIARY'), 35),\n",
       " (('said', 'Mrs.', 'Dobbs'), 34),\n",
       " (('Lady', 'Laura', 'Standish'), 33),\n",
       " (('said', 'Mr.', 'Bragg'), 33),\n",
       " (('said', 'Mr.', 'Shelby'), 32),\n",
       " (('said', 'Mr.', 'Edmonstone'), 32),\n",
       " (('Mrs.', 'Machyn', 'Stubbs'), 32),\n",
       " (('young', 'man', 'said'), 31),\n",
       " (('said', 'Mr', 'Harding'), 31),\n",
       " (('said', 'Mrs.', 'Pullet'), 31)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root = r'..\\datasets\\2_txtalb_Novel450'\n",
    "files = [os.path.join(data_root, f) for f in os.listdir(data_root) if f.startswith('EN_')][60:110]\n",
    "        \n",
    "def all_ngrams(files):\n",
    "    texts = (preprocess.read_pg(filename)[:nlp.max_length] for filename in files)\n",
    "    counter = Counter()\n",
    "    disabled = ['tagger','parser','ner','entity_ruler','sentencizer','merge_entities']\n",
    "    for doc in [nlp(txt, disable=disabled) for txt in texts]:#(nlp.pipe(texts, batch_size=2, disable=disabled)):\n",
    "        c = process_rgrams(doc)\n",
    "        counter += c.reg_grams_count[2]\n",
    "    return counter\n",
    "\n",
    "c = all_ngrams(files)\n",
    "c.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[:20] for x in (preprocess.read_pg(filename)[:nlp.max_length] for filename in files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_cnt = Counter(t.dep_ for t in doc)\n",
    "dep_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dep_cnt = Counter(t.dep_ for t in doc if t.dep_ not in FUNCTIONAL_DEPS)\n",
    "sum(content_dep_cnt.values()), sum((dep_cnt - content_dep_cnt).values()), sum((dep_cnt).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "sents = {}\n",
    "current_sent_start = None\n",
    "for gram in islice(grams, 50):\n",
    "    sent = gram[0].sent\n",
    "    if sent.start != current_sent_start:\n",
    "        current_sent_start = sent.start\n",
    "        print(current_sent_start, \" \".join(t.text for t in sent if not t.is_space))\n",
    "        sents[current_sent_start] = sent\n",
    "    gram_pos = [t.pos_ for t in gram]\n",
    "    gram_dep = [t.dep_ for t in gram]\n",
    "    print(gram)\n",
    "    print(\" \"*6, gram_dep)\n",
    "    print(\" \"*6, gram_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(sents[3], style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The family of Dashwood had long been settled in Sussex .\"\n",
    "['settle', 'family', 'of', 'Dashwood']\n",
    "['settle', 'in', 'Sussex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sents[3][7].ancestors), sents[3][7].dep_\n",
    "list(sents[3][3].ancestors), sents[3][7].dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sents[3].root.subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ancestors(tok):\n",
    "    while tok.head != tok:\n",
    "        tok = tok.head\n",
    "        yield tok\n",
    "    if tok.head != tok:\n",
    "        yield tok\n",
    "\n",
    "print(\"--- custom:\")\n",
    "for t in sents[3]:\n",
    "    print(list(ancestors(t))[::-1], t.dep_, t)\n",
    "\n",
    "print(\"--- spacy:\")\n",
    "for t in sents[3]:\n",
    "    print(list(t.ancestors)[::-1], t.dep_, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(\"He didn't even look at her.\"), style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a.insert(0, 42)\n",
    "a.insert(0, 43)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(2,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(t.text, t.dep_, t.pos_) for t in nlp('Alice was going up the stairs.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 4\n",
    "for i in range(2, x):\n",
    "    print(i)\n",
    "else:\n",
    "    print('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(r'..\\datasets\\2_txtalb_Novel450') if f.startswith('EN_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([f for f in os.listdir(r'..\\datasets\\2_txtalb_Novel450') if f.startswith('EN_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:.6f}s\".format(time.perf_counter() - time.perf_counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nlp.pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k for k in globals().keys() if not k.startswith('_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Toto' object has no attribute 'a'\n",
      "42 43\n"
     ]
    }
   ],
   "source": [
    "class Toto:pass\n",
    "\n",
    "t = Toto()\n",
    "t.a = 42\n",
    "t2 = Toto()\n",
    "try:\n",
    "    print(t.a, t2.a)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "t2.a = 43\n",
    "print(t.a, t2.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08667145,  0.96247106, -0.90505545, -0.67014953,  0.68932609,\n",
       "        -0.08000127],\n",
       "       [-0.9814764 , -0.01658201,  0.45627636,  0.13262529,  0.79202124,\n",
       "        -0.55723361],\n",
       "       [ 0.94989781, -0.47164804,  0.06555926, -0.21102938, -0.27916941,\n",
       "         0.94720727],\n",
       "       [ 0.68735926, -0.14219569,  0.20014977,  0.05008628,  0.39812897,\n",
       "         0.89753264],\n",
       "       [-0.74043738, -0.79034741,  0.2726128 , -0.38621711,  0.00357159,\n",
       "         0.61631072],\n",
       "       [ 0.0760054 ,  0.51077689, -1.01802278, -0.94650462, -0.63429054,\n",
       "        -0.00322389]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.triu(np.random.random([6,6])*2-1)\n",
    "x-x.T*(1+np.random.random([6,6])*.2-.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
